# ── Local LLM configuration ─────────────────────────────────────────────────
# Copy this file to .env and edit as needed.  docker-compose.yml reads it.
# Usage: cp env.example .env

# Base URL of the OpenAI-compatible endpoint exposed by Ollama (or any other
# compatible server running locally).
# Default: http://localhost:11434/v1/
LLM_ENDPOINT_URL=http://localhost:11434/v1/

# Model to use.  Must be available in the Ollama instance.
# Run `docker compose --profile init run ollama-init` to pull the model first.
# Any model from https://ollama.com/library works (e.g. llama3.2, phi3, mistral).
MODEL_NAME=tinyllama

# API key sent to the endpoint.  Most local servers (including Ollama) ignore
# this value, but the field must be non-empty.
LLM_API_KEY=not-required
