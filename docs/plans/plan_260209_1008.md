# Plan: NVIDIA NIM Podcast Asset Generation for Scenario 4

**Date**: 2026-02-09  
**Status**: Draft

## TL;DR

Add a podcast asset generation feature to Scenario 4 that uses an NVIDIA NIM LLM container (running locally via Docker) to generate episode titles, descriptions, and tags from transcription text. The NIM container joins the Aspire orchestration alongside the existing ASR server. Both the Blazor web app (new dedicated "Podcast Assets" page) and the console client (`--generate-assets` flag) call the NIM container directly via its OpenAI-compatible API. The web app also supports pasting a raw transcript for asset generation without needing to transcribe first. Multiple NIM model options are proposed for different GPU tiers (8–24GB VRAM), with a default targeting consumer 8–12GB cards.

---

## NIM Model Recommendations

Since the ASR server (Parakeet 0.6B, ~1.5GB VRAM) already occupies the GPU, LLM model choice must account for shared VRAM:

| Model | Params | VRAM Needed | GPU Tier | Notes |
|---|---|---|---|---|
| `nvidia/llama-3.2-nv-minitron-4b-instruct` | 4B | ~6–8GB | 8–12GB (RTX 3060/4060) | **Default.** NVIDIA-optimized, best for consumer GPUs sharing VRAM with ASR |
| `mistralai/mistral-7b-instruct-v0.3` | 7B | ~12–14GB | 16GB+ (RTX 4070 Ti) | Better quality output, needs dedicated GPU or larger card |
| `meta/llama-3.1-8b-instruct` | 8B | ~14–16GB | 24GB (RTX 3090/4090) | Best quality for single-GPU systems |

**Recommendation**: Default to `nvidia/llama-3.2-nv-minitron-4b-instruct` as it can coexist with Parakeet on a 12GB GPU (~1.5GB ASR + ~8GB LLM). Make the model configurable via Aspire AppHost or environment variable so users can swap in a larger model if they have more VRAM or a second GPU.

---

## Architecture Decisions

- **Direct client-to-NIM**: Clients call NIM directly (not proxied through Python server), keeping the surfaces independent and avoiding Python server becoming a bottleneck for LLM calls
- **Default model**: `nvidia/llama-3.2-nv-minitron-4b-instruct` (4B) as default for consumer GPU (8–12GB) compatibility
- **Configurable image**: NIM container image is configurable so users can swap in larger models without code changes
- **OpenAI-compatible SDK**: Use the `OpenAI` NuGet package with custom base URL rather than raw HTTP, for cleaner code and future compatibility
- **NGC API key required**: NIM containers require NGC credentials; documented in setup instructions

---

## Implementation Steps

### Step 1 — Add NIM container to Aspire orchestration

In `scenario4/AppHost/Program.cs`, add a new NIM container resource:
- Use `AddContainer()` with image `nvcr.io/nim/nvidia/llama-3.2-nv-minitron-4b-instruct` (make image configurable via parameter or config)
- Expose port 8000 for the OpenAI-compatible API
- Pass `--gpus=all` via `WithContainerRuntimeArgs` (same pattern as the ASR server)
- Add a persistent Docker volume for NIM model cache at `/opt/nim/.cache`
- Inject `NGC_API_KEY` environment variable (required for NIM; read from Aspire configuration/secrets)
- Add health check on `/v1/health/ready`
- Set `ContainerLifetime.Persistent` (same as ASR server — model stays loaded)
- Wire the NIM endpoint to both the web app and console client via service references

### Step 2 — Add ServiceDefaults for NIM client

In `scenario4/ServiceDefaults/`, add a helper extension method or configuration:
- Register a named `HttpClient` (e.g., `"nim"`) pointing to the NIM container
- Same resilience settings as the ASR client (5-min timeout per attempt, since first-load model warmup can be slow)
- Alternatively, use the `OpenAI` NuGet package with custom base URL pointing to the NIM service discovery endpoint

### Step 3 — Create shared prompt templates

Create a new file `scenario4/shared/PodcastPrompts.cs` (or embed in each client) containing the system/user prompt templates for asset generation:
- **System prompt**: "You are a podcast production assistant. Given a transcript, generate a concise episode title, an engaging episode description (2–3 sentences), and 5–8 relevant tags."
- **User prompt template**: "Here is the transcript:\n\n{transcript}\n\nGenerate the following in JSON format:\n- title\n- description\n- tags (array of strings)"
- Request JSON output with `response_format: { type: "json_object" }` for structured parsing

### Step 4 — Add "Podcast Assets" page to web app

Create a new Blazor page `scenario4/clients/webapp/Components/Pages/PodcastAssets.razor`:
- **Route**: `/podcast-assets`
- **Input section** with two modes:
  - **Paste mode**: A `<textarea>` where the user pastes a transcript directly
  - **From transcription**: Accept transcript text passed via query parameter or navigation state from the Transcribe page (add a "Generate Podcast Assets" button on the Transcribe results view)
- **Generate button**: Calls the NIM container's `/v1/chat/completions` endpoint using the OpenAI-compatible client
- **Results section**: Displays the generated title, description, and tags in styled cards
- **Copy/export**: Copy individual fields or export all as JSON
- Follow existing Blazor patterns from `Transcribe.razor` (progress log panel, responsive design, `@rendermode InteractiveServer`)

### Step 5 — Create NIM client service for web app

Create `scenario4/clients/webapp/Services/NimAssetService.cs`:
- Use the `OpenAI` NuGet package (or `Microsoft.Extensions.AI`) with a custom `OpenAIClient` pointing at the NIM service URL (resolved via Aspire service discovery: `http://nim-llm`)
- Method `GeneratePodcastAssetsAsync(string transcript)` that:
  - Sends the prompt to `/v1/chat/completions`
  - Parses the JSON response into a `PodcastAssets` record (Title, Description, Tags)
  - Handles errors/timeouts gracefully
- Register in `Program.cs` with the NIM endpoint from Aspire

### Step 6 — Wire up navigation in web app

Update `scenario4/clients/webapp/Components/Layout/NavMenu.razor`:
- Add a "Podcast Assets" nav link pointing to `/podcast-assets`

Update `scenario4/clients/webapp/Components/Pages/Transcribe.razor`:
- After transcription completes, add a "Generate Podcast Assets" button in the results section
- The button navigates to `/podcast-assets` passing the transcript text (via `NavigationManager` with state or query parameter)

### Step 7 — Add asset generation to console client

Update `scenario4/clients/console/Program.cs`:
- Add `--generate-assets` CLI flag
- After transcription completes (or standalone with `--transcript-file <path>`), call the NIM endpoint directly
- Use `HttpClient` to POST to `http://<nim-endpoint>/v1/chat/completions` with the same prompt template
- Parse JSON response and display title, description, tags in formatted console output
- Add NIM endpoint parameter: `--nim-url` with default from env var `services__nim-llm__http__0` (Aspire service discovery pattern)

### Step 8 — Add NuGet packages

Update `scenario4/clients/webapp/TranscriptionWebApp2.csproj` and `scenario4/clients/console/TranscriptionClient.csproj`:
- Add `OpenAI` NuGet package (the official `openai` package supports custom base URIs, making it compatible with NIM's OpenAI-compatible API)
- Or use `Microsoft.Extensions.AI.OpenAI` for the Microsoft AI abstractions

### Step 9 — Update AppHost project references

In `scenario4/AppHost/Program.cs`:
- Wire `nimServer` reference to both the web app and console projects using `.WithReference(nimServer)`
- Ensure the web app `WaitFor(nimServer)` alongside the ASR server

### Step 10 — Update documentation

- Update `scenario4/README.md` with the new feature, NIM setup requirements (NGC API key), and GPU VRAM guidance
- Update `scenario4/docs/ARCHITECTURE.md` with the new NIM container in the architecture diagram
- Add NGC API key setup instructions (sign up at `build.nvidia.com`, generate key, configure in Aspire user secrets)

---

## Verification

1. **NIM container startup**: Run the Aspire AppHost and verify the NIM container pulls, starts, and passes health check (`/v1/health/ready` goes green in Aspire dashboard)
2. **Web app paste mode**: Navigate to Podcast Assets page → paste a sample transcript → click Generate → verify title, description, and tags are returned
3. **Web app flow-through**: Transcribe an audio file → click "Generate Podcast Assets" on results → verify transcript is passed to the new page and assets are generated
4. **Console client**: Run `TranscriptionClient.exe test.mp3 --generate-assets` → verify assets print after transcription
5. **Console standalone**: Run `TranscriptionClient.exe --generate-assets --transcript-file transcript.txt` → verify assets generated from file
6. **GPU memory**: Monitor `nvidia-smi` during concurrent ASR + LLM usage to confirm both models fit in VRAM
